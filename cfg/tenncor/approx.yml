name: ApproxAPI
members:
  - name: super
    type: TenncorAPI*
init:
  args:
    - name: super
      type: TenncorAPI*
  initlist:
    super: super
funcs:
  - template: typename T
    name: sgd
    description: |
      Return all batches of variables to their corresponding assignment operator
      applied using stochastic gradient descent approximation
      given the error tenncor node and their corresponding variables

      :::::: Stochastic Gradient Descent Optimization ::::::

      Let f be the error
      For each variable x in variables

      x_next = x - learning_rate * f'(x)
    args:
      - name: error
        type: const eteq::ETensor&
      - name: variables
        type: const eteq::EVariablesT<T>&
      - name: learning_rate
        type: T
        default: 0.5
      - name: apply
        type: layr::UnaryF
        default: layr::UnaryF()
    out:
      type: layr::VarErrsT<T>
      val: |
        //
            layr::VarErrsT<T> out;
            auto ders = tcr::derive(error,
                eteq::ETensorsT(variables.begin(),variables.end()));
            for (size_t i = 0,n = variables.size(); i < n; ++i)
            {
                auto& x = variables[i];
                auto& der = ders[i];
                if (apply)
                {
                    der = apply(der);
                }

                out.push_back({x,super->assign_sub(x,super->mul(der,learning_rate))});
            }
            return out;
  - template: typename T
    name: adagrad
    args:
      - name: error
        type: const eteq::ETensor&
      - name: variables
        type: const eteq::EVariablesT<T>&
      - name: learning_rate
        type: T
        default: 0.5
      - name: epsilon
        type: T
        default: std::numeric_limits<T>::epsilon()
      - name: apply
        type: layr::UnaryF
        default: layr::UnaryF()
    out:
      type: layr::VarErrsT<T>
      val: |
        //
            layr::VarErrsT<T> out;
            auto ders = tcr::derive(error,
                eteq::ETensorsT(variables.begin(),variables.end()));
            for (size_t i = 0,n = variables.size(); i < n; ++i)
            {
                auto& x = variables[i];
                auto& der = ders[i];
                if (apply)
                {
                    der = apply(der);
                }

                eteq::EVariable<T> momentum = eteq::make_variable_like<T>(1,der,"momentum",super->ctx);
                auto update = super->assign_add(momentum,super->square(der));

                // assign momentums before leaves
                out.push_back({x,super->assign_sub(x,super->div(
                    super->mul(der,learning_rate),
                    super->add(super->sqrt(update),epsilon)))});
            }
            return out;
  - template: typename T
    name: adadelta
    description: |
      Return all batches of variables to their corresponding assignment operator
      applied using adadelta error optimization
      given the error tenncor node and their corresponding variables

      :::::: Adadelta Optimization ::::::

      Let f be the error,g be short hand for f'(x)
      Initialize E(g^2) = 0,E(dx^2) = 0
      For each variable x in variables

      E(g_next^2) = decay * E(g^2) + (1 - decay) * f'(x) ^ 2
      dx = step_rate * RMS(dx + offset) / (RMS(g_next + offset)) * f'(x)
        = step_rate * sqrt(E(dx^2) + offset) / sqrt(E(g_next^2) + epsilon) * f'(x)

      E(dx_next^2) = decay * E(dx^2) + (1 - decay) * dx ^ 2
      x_next = x - dx
    args:
      - name: error
        type: const eteq::ETensor&
      - name: variables
        type: const eteq::EVariablesT<T>&
      - name: step_rate
        type: T
        default: 1
      - name: decay
        type: T
        default: 0.9
      - name: offset
        type: T
        default: 0.0001
      - name: apply
        type: layr::UnaryF
        default: layr::UnaryF()
    out:
      type: layr::VarErrsT<T>
      val: |
        //
            layr::VarErrsT<T> out;
            auto ders = tcr::derive(error,
                eteq::ETensorsT(variables.begin(),variables.end()));
            for (size_t i = 0,n = variables.size(); i < n; ++i)
            {
                auto& x = variables[i];
                auto& der = ders[i];
                if (apply)
                {
                    der = apply(der);
                }

                T nodecay = 1. - decay;
                eteq::EVariable<T> msg = eteq::make_variable_like<T>(0,der,"ex_sqr_grad",super->ctx);
                eteq::EVariable<T> msd = eteq::make_variable_like<T>(0,der,"ex_sqr_delx",super->ctx);

                auto msg_next = super->assign(msg,
                    super->add(
                        super->mul(decay,msg),
                        super->mul(nodecay,super->square(der))
                    )
                );
                auto delta = super->mul(
                    super->mul(step_rate,super->div(
                        super->sqrt(super->add(msd,offset)),
                        super->sqrt(super->add(msg_next,offset))
                    )),der);

                auto msd_next = super->assign(msd,
                    super->add(
                        super->mul(decay,msd),
                        super->mul(nodecay,super->square(delta))
                    )
                );
                out.push_back({x,
                    super->assign_sub(x,super->identity(delta, {msd_next}))
                });
            }
            return out;
  - template: typename T
    name: rms_momentum
    description: |
      Return all batches of variable assignments of
      momentum-based rms error approximation applied to
      particular variables-error associations

      :::::: Momentum-based Root Mean Square Optimization ::::::

      Let f be the error
      Initialize m = 1
      For each x in leaves

      momentum_next = discount_factor * m + (1 - discount_factor) * f'(x) ^ 2
      x_next = x - (learning * f'(x)) / (sqrt(epsilon + momentum_next))
    args:
      - name: error
        type: const eteq::ETensor&
      - name: variables
        type: const eteq::EVariablesT<T>&
      - name: learning_rate
        type: T
        default: 0.5
      - name: discount_factor
        type: T
        default: 0.99
      - name: epsilon
        type: T
        default: std::numeric_limits<T>::epsilon()
      - name: apply
        type: layr::UnaryF
        default: layr::UnaryF()
    out:
      type: layr::VarErrsT<T>
      val: |
        //
            layr::VarErrsT<T> out;
            auto ders = tcr::derive(error,
                eteq::ETensorsT(variables.begin(),variables.end()));
            for (size_t i = 0,n = variables.size(); i < n; ++i)
            {
                auto& x = variables[i];
                auto& der = ders[i];
                if (apply)
                {
                    der = apply(der);
                }

                eteq::EVariable<T> momentum = eteq::make_variable_like<T>(
                    1,der,"momentum",super->ctx);
                auto update = super->assign(momentum,
                    super->add(
                        super->mul(discount_factor,momentum),
                        super->mul(1-discount_factor,
                            super->square(der))));

                // assign momentums before leaves
                out.push_back({x,super->assign_sub(x,
                    super->div(
                        super->mul(der,learning_rate),
                        super->add(super->sqrt(update),epsilon)))});
            }
            return out;
