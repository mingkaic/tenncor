{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "import ead.age as age\n",
    "import ead.ead as ead\n",
    "import rocnnet.rocnnet as rcn\n",
    "\n",
    "from tf_rl.controller import HumanController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/fs/cfs_ql391csfpxfsdxhmktw00000gn/T/tmpAe42L3\n"
     ]
    }
   ],
   "source": [
    "LOG_DIR = tempfile.mkdtemp()\n",
    "print(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_settings = {\n",
    "    'objects': [\n",
    "        'friend',\n",
    "        'enemy',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'hero':   'yellow',\n",
    "        'friend': 'green',\n",
    "        'enemy':  'red',\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'friend': 0.1,\n",
    "        'enemy': -0.1,\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (700,500),\n",
    "    'hero_initial_position': [400, 300],\n",
    "    'hero_initial_speed':    [0,   0],\n",
    "    \"maximum_speed\":         [50, 50],\n",
    "    \"object_radius\": 10.0,\n",
    "    \"num_objects\": {\n",
    "        \"friend\" : 25,\n",
    "        \"enemy\" :  25,\n",
    "    },\n",
    "    \"num_observation_lines\" : 32,\n",
    "    \"observation_line_length\": 120.,\n",
    "    \"tolerable_distance_to_wall\": 50,\n",
    "    \"wall_distance_penalty\":  -0.0,\n",
    "    \"delta_v\": 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNControllerWrapper:\n",
    "    def __init__(self, trainer):\n",
    "        self.trainer = trainer\n",
    "\n",
    "    def action(self, o):\n",
    "        return self.trainer.action(o)\n",
    "\n",
    "    def store(self, observation, action, reward, newobservation):\n",
    "        self.trainer.store(observation, action, reward, newobservation)\n",
    "\n",
    "    def training_step(self):\n",
    "        self.trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot perform SUB with incompatible shapes [200\\1\\1\\1\\1\\1\\1\\1] and [5\\200\\1\\1\\1\\1\\1\\1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-10505813da4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mcurrent_controller\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNControllerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrcn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDQNTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot perform SUB with incompatible shapes [200\\1\\1\\1\\1\\1\\1\\1] and [5\\200\\1\\1\\1\\1\\1\\1]"
     ]
    }
   ],
   "source": [
    "human_control = False\n",
    "\n",
    "if human_control:\n",
    "    # WSAD CONTROL (requires extra setup - check out README)\n",
    "    current_controller = HumanController({b\"w\": 3, b\"d\": 0, b\"s\": 1,b\"a\": 2,}) \n",
    "else:\n",
    "    # Brain maps from observation to Q values for different actions.\n",
    "    # Here it is a done using a multi layer perceptron with 2 hidden\n",
    "    # layers\n",
    "    hiddens = [\n",
    "        rcn.get_layer(age.tanh, 200),\n",
    "        rcn.get_layer(age.tanh, 200),\n",
    "        rcn.get_layer(rcn.identity, g.num_actions)\n",
    "    ]\n",
    "    brain = rcn.get_mlp(g.observation_size, hiddens, 'brain')\n",
    "    \n",
    "    # todo: use rms_prop instead\n",
    "    bgd = rcn.get_rms_momentum(\n",
    "        learning_rate = 0.001,\n",
    "        discount_factor = 0.9)\n",
    "    \n",
    "    param = rcn.get_dqninfo(\n",
    "        discount_rate = 0.99,\n",
    "        exploration_period = 5000,\n",
    "        max_exp = 1000,\n",
    "        store_interval = 4,\n",
    "        train_interval = 4)\n",
    "    \n",
    "    sess = ead.Session()\n",
    "    current_controller = DQNControllerWrapper(rcn.DQNTrainer(brain, sess, bgd, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "# fast_mode = True\n",
    "fast_mode = False\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 50\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "    \n",
    "try:\n",
    "    simulate(\n",
    "        simulation=g,\n",
    "        controller=current_controller,\n",
    "        fps=FPS,\n",
    "        visualize_every=VISUALIZE_EVERY,\n",
    "        action_every=ACTION_EVERY,\n",
    "        wait=WAIT,\n",
    "        disable_training=False,\n",
    "        simulation_resolution=0.001,\n",
    "        save_path=None)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_controller.q_network.input_layer.Ws[0].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_controller.target_q_network.input_layer.Ws[0].eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Reward over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.plot_reward(smoothing=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_controller.q_network.input_layer.Ws[0].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_controller.target_q_network.input_layer.Ws[0].eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing what the agent is seeing\n",
    "\n",
    "Starting with the ray pointing all the way right, we have one row per ray in clockwise order.\n",
    "The numbers for each ray are the following:\n",
    "- first three numbers are normalized distances to the closest visible (intersecting with the ray) object. If no object is visible then all of them are $1$. If there's many objects in sight, then only the closest one is visible. The numbers represent distance to friend, enemy and wall in order.\n",
    "- the last two numbers represent the speed of moving object (x and y components). Speed of wall is ... zero.\n",
    "\n",
    "Finally the last two numbers in the representation correspond to speed of the hero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.__class__ = KarpathyGame\n",
    "np.set_printoptions(formatter={'float': (lambda x: '%.2f' % (x,))})\n",
    "x = g.observe()\n",
    "new_shape = (x[:-4].shape[0]//g.eye_observation_size, g.eye_observation_size)\n",
    "print(x[:-4].reshape(new_shape))\n",
    "print(x[-4:])\n",
    "g.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
